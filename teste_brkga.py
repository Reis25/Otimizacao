# -*- coding: utf-8 -*-
"""teste_brkga.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wXiQNQ_nmotkOpMW4h5VHILOyLz1tlzL
"""

#%%
import numpy as np
import pandas as pd
#from prediction import predict_model, metrics_by_model

class GeneticSelection:
    ''' Genetic algorithm to feature selection
    
    Attributes
    ----------
    random : instance of numpy random state
    
    n_population : int
        Number of individuals on the population
        
    n_generations : int
        Max number of generations
        
    tournament_size : int
        Size of tournament selection method
        
    crossover_proba : float
        Probability of changing chromossome points in uniform crossover
        
    mutation_proba : float
        Simple probability on mutating a chromossome in a individual
        
    n_gen_no_change : int
        Max number of generations without change it's best value
        
    scoring : str
        Metric to be optimized
        ['accuracy', 'average_precision', 'f1', 'ppv', 'tpr', 'tnr', 'roc_auc']
    
    '''
    def __init__(self, random_state=0, scoring='roc_auc', n_population=50, n_generations=100,
                 elite_solutions_size=10, n_gen_no_change=10, elite_inheritance_proba=0.75, mutants_solutions_size=35):
        self.random = np.random.RandomState(random_state) if isinstance(random_state, int) else random_state
        self.n_population = n_population
        self.n_generations = n_generations
        self.n_gen_no_change = n_gen_no_change
        self.scoring = scoring
        self.elite_solutions_size = elite_solutions_size
        self.mutants_solutions_size = mutants_solutions_size
        self.elite_inheritance_proba = elite_inheritance_proba

    def fit(self, base, model):
        ''' Function that starts the genetic feature selection        
        Parameters
        ----------
        base : tuple (<PANDAS DF>, <PANDAS SERIES>)
            where the first value is a pandas dataframe with non class features of a
            dataset, the second value correspond to class value for each sample.
        model : classification model instance
        
        Returns : list
            Each element on the list is a tuple (<FEATURES>, <fitness_score>)
            the first one is a binary list of features to be used (1) or not (0)
            the second tuple value is the fitness value of the features list
	
  matrix_graph = [[0 for x in range(10) for y in range(10)]
	passando os parametros do grafo
	passamos o grafo
        '''
    lista_de_vertice = ['a', 'b', 'c', 'd', 'e', 'f']

    lista_de_aresta = [('a', 'b'), ('b', 'a'), ('b', 'c'), ('c', 'b'), ('c', 'e'), ('e', 'a'), ('e', 'b'), ('e', 'c'), ('e', 'd'), ('d', 'a'), ('d', 'e'), ('e', 'f'), ('f', 'e'), ('b', 'e'), ('a', 'd'), ('a', 'e')]
  
    def cria_grafo(lista_de_vertices, lista_de_arestas):
      grafo = {}
      for vertice in lista_de_vertices:
          grafo[vertice] = []
      for aresta in lista_de_arestas:
          grafo[aresta[0]].append(aresta[1])
      return grafo
	
    def BFS(self, s): 
        visited = [False] * (len(self.graph)) 
  
        queue = [] 
        queue.append(s) 
        visited[s] = True
  
        while queue: 
            s = queue.pop(0) 
            print (s, end = " ") 

            for i in self.graph[s]: 
                if visited[i] == False: 
                    queue.append(i) 
                    visited[i] = True
  
	
    (X, _) = base

    if type(X) is type(pd.DataFrame()):
        individual_size = len(X.iloc[0, :].values)
    else:
        individual_size = len(X[0])

    self.individual_size = individual_size
    self.individual_fitness_map = {}

    print("----- Starting population -----")
    initial_population = self.generate_individuals(base, model, self.n_population)
    final_population = self.generations(base, model, initial_population)
        
      return final_population

    def generations(self, base, model, population):
        ''' Applies generations over the initial population, returns the final population
        of individuals
        
        Parameters
        ----------
        base : tuple (<PANDAS DF>, <PANDAS SERIES>)
            where the first value is a pandas dataframe with non class features of a
            dataset, the second value correspond to class value for each sample.
        model : classification model instance
        population : list
            a list of binary lists, individuals to be optimized through the generations
        
        Returns
        -------
        population : list
            a list of binary lists, individuals optimized through the generations
        '''
        gen_no_change = 0
        population.sort(key=self.fitness_score, reverse=True)
        best_individual = population[0]
        for generation in range(self.n_generations):
            print("--- Running generation %d of %d ---"%(generation+1, self.n_generations))
            elite, non_elite = self.selection(population)
            new_mutants = self.generate_individuals(base, model, self.mutants_solutions_size)
            new_children_size = self.n_population - len(elite) - len(new_mutants)
            new_children = self.breeding(best_individual, population[1:-1], new_children_size)
            population = elite + new_children + new_mutants
            assert len(population) == self.n_population
            for individual in population:
                self.evaluation(base, model, individual)
            population.sort(key=self.fitness_score, reverse=True)
            new_best_individual, new_best_fitness = (population[0], 
                                                     self.fitness_score(population[0]))
            print("Best individual: %s | Score: %s"%(new_best_individual, new_best_fitness))
            if not str(np.array(new_best_individual)) == str(np.array(best_individual)):
                best_individual = new_best_individual
                gen_no_change = 0
            else:
                gen_no_change = gen_no_change + 1
            if gen_no_change > self.n_gen_no_change:
                break
        return population

    def selection(self, population):
        ''' Applies tournament method to select individuals from the population        
        '''
        selected_individuals = population[0:self.elite_solutions_size]
        non_selected_individuals = population[self.elite_solutions_size:-1]
        return selected_individuals, non_selected_individuals

    def crossover(self, parent_elite, parent_other):
        ''' Applies uniform crossover method over two individuals to generate
        other two individuals
        '''
        child = []
        for i in range(len(parent_elite)):
            if self.random.rand() > self.elite_inheritance_proba:
                child.append(parent_other[i])
            else:
                child.append(parent_elite[i])
        return child

    def generate_individuals(self, base, model, num_individuals):
        population = []
        while len(population) < num_individuals:
            individual = self.random.rand(self.individual_size)
            self.evaluation(base, model, individual)
            population.append(individual)
        return population

    def breeding(self, parent_a, parent_candidates, children_size):
        ''' Creates children individuals through parents crossovers and mutation of its childs
        
        Parameters
        ----------
        parents : list
            a list of binary lists
            
        Raises
        ------
        'Parents number exception' when the parents number is lower than 2
        
        Returns
        -------
        children : list
            a list of binary lists of individuals generated by crossovers of the parents 
            individuals
        '''
        n_parents = len(parent_candidates)
        children = []
        while len(children) < children_size:
            p1 = self.random.randint(0, n_parents)
            child = self.crossover(parent_a, parent_candidates[p1])
            children.append(child)
        return children

    def fitness_score(self, individual):
        ''' Function to get the fitness score, works through a individual-fitness map
        
        Parameters
        ----------
        individual : list
        
        Raises
        ------
        'Fitness not calculated' exception when the individual has not a related 
        evaluation on map
        
        Returns
        -------
        fitness : float

        '''

        try:
            features = self.encodeThreshold(individual)
            fitness = self.individual_fitness_map[str(features)]
        except:
            raise Exception('Fitness not calculated')
        return fitness


    def evaluation(self, base, model, individual):
        ''' Function to calculate the fitness score of a individual and save it in a map
        
        Parameters
        ----------
        base : tuple (<PANDAS DF>, <PANDAS SERIES>)
            where the first value is a pandas dataframe with non class features of a
            dataset, the second value correspond to class value for each sample.
        model : classification model instance
        individual : list
        
        Raises
        ------
        'Fitness could not be calculated' exception when the individual could not be calculated
        
        Returns
        -------
        individual : list
        '''
        features = self.encodeThreshold(individual)
        if str(features) in self.individual_fitness_map.keys():
            fitness = self.individual_fitness_map[str(features)]
        else:
            fitness = self.evaluate(base, model, features)
            self.individual_fitness_map[str(features)] = fitness
        return individual

    def encodeThreshold(self, features):
        return [ 1 if feature > 0.5 else 0 for feature in features ]

    def evaluate(self, base, model, features):
        ''' Function to calculate the fitness score of a individual 
        
        Parameters
        ----------
        base : tuple (<PANDAS DF>, <PANDAS SERIES>)
            where the first value is a pandas dataframe with non class features of a
            dataset, the second value correspond to class value for each sample.
        model : model classifier instance
        individual : list
    
        
        Returns
        -------
        score : float
        '''
        predictions = predict_model(base, model, features=features)
        score = metrics_by_model(predictions)[self.scoring].mean()
        return score

obj = GeneticSelection.cria_grafo(lista_de_vertice, lista_de_aresta)

obj

from sklearn.metrics import roc_curve, auc
from sklearn.metrics import accuracy_score
from sklearn.metrics import average_precision_score
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
import time
import numpy as np
import pandas as pd
from sklearn.model_selection import RepeatedStratifiedKFold
import matplotlib.pyplot as plt
from errno import EEXIST
from os import makedirs, path
from scipy import interp


def predict_model(base, model, features=None, random_state=0, k_folds=10, n_repeats=5):
    ''' Train and test a prediction model with crossvalidation
    
    Parameters
    ----------
    base : tuple (<PANDAS DF>, <PANDAS SERIES>)
        where the first value is a pandas dataframe with non class features of a
        dataset, the second value correspond to class value for each sample.
    model : classification model instance
    features : list
        A binary list of features 1 refers to presence of a feature, 0 refers to
        it's absense
    sampling : sampling class instance
    kfolds : int
    n_repeats : int
    Returns
    -------
    dict[<BASE_NAME>][<MODEL_NAME>]
        [<FEATURES>] : Pandas dataframe
            A dataframe with features of the model prediction
        [<KFOLDS>][<KFOLD>]
            [<Y_TRUE>] : list
                True values of test kfold samples
            [<Y_PRED>] : list
                Predicted values of test kfold samples
            [<IMPORTANCES>] : Pandas dataframe
                Gini importance of features for each kfold
 
    '''

    random = np.random.RandomState(random_state) if random_state is int() else random_state
    rskf = RepeatedStratifiedKFold(n_splits=k_folds, random_state=random, n_repeats=n_repeats)
    # sampling = SMOTE(random_state=random)
    X, y = base
    if features is None:
        features = [1 for val in X[0]]
    selected_features = [True if val == 1 else False for val in features]
    X = X[:, selected_features]
    predictions_dict = {'kfolds': {}}
    start_time = time.time()

    for i, (train, test) in enumerate(rskf.split(X, y)):
        X_train, y_train = (X[train], y[train])
        X_test, y_test = (X[test], y[test])
        model.fit(X_train, y_train)
        y_pred = model.predict_proba(X_test)[:, 1]
        predictions_dict['kfolds']['rep%dkf%d' % (i // n_repeats, i)] = {
            'y_true': y_test,
            'y_pred': y_pred
        }

    classification_time = time.time() - start_time
    return predictions_dict


def metrics_by_prediction(y_true, y_pred):
    ''' Applies metrics to comparisson of predicted and true values
    
    Parameters
    ----------
    y_true : list
        True binary labels or binary label indicators.
    y_pred : list
        Target scores, can either be probability estimates of the positive class, 
        confidence values, or non-thresholded measure of decisions.
    
    Returns
    -------
    dict
        a dict of int, float, list, representing each calculated metric
    '''
    metrics = {}
    y_bin = [1 if feature >= 0.5 else 0 for feature in y_pred]

    accuracy = accuracy_score(y_true, y_bin)
    average_precision = average_precision_score(y_true, y_pred)
    conf_matrix = confusion_matrix(y_true, y_bin)
    f1 = f1_score(y_true, y_bin)
    fpr_roc, tpr_roc, _ = roc_curve(y_true, y_pred)
    roc_auc = auc(fpr_roc, tpr_roc)

    tn, fp, fn, tp = conf_matrix.ravel()

    metrics['accuracy'] = accuracy
    metrics['average_precision'] = average_precision
    metrics['f1'] = f1
    metrics['fp'] = fp
    metrics['fn'] = fn
    metrics['tp'] = tp
    metrics['tn'] = tn
    metrics['ppv'] = tp / (tp + fp)
    metrics['tpr'] = tp / (tp + fn)
    metrics['tnr'] = tn / (tn + fp)
    metrics['fpr_roc'] = fpr_roc
    metrics['tpr_roc'] = tpr_roc
    metrics['roc_auc'] = roc_auc

    return metrics


def metrics_by_model(model_pred):
    ''' Organizes the metrics for each kfold and saves it in a file

    Parameters
    ----------
    model_pred : dict
        A dict that follows prediction[<FOLD>][<METRICS>] : int, float, list
    write : bool
        Defines if the metrics should be saved in a file or not
    path : str
    file_name : str

    The path of the file is <PATH>/metrics/<FILE_NAME>.csv
    '''
    metrics_dict = {}
    kfolds_pred = model_pred['kfolds']

    for kfold in kfolds_pred:
        metrics = metrics_by_prediction(kfolds_pred[kfold]['y_true'],
                                        kfolds_pred[kfold]['y_pred'])
        for metric in metrics:
            value = metrics[metric]
            if isinstance(value, int) or isinstance(value, float):
                if metric not in metrics_dict.keys():
                    metrics_dict[metric] = []
                metrics_dict[metric].append(value)

    metrics_dataframe = pd.DataFrame.from_dict(metrics_dict)
    return metrics_dataframe


def plot_roc_auc(model_pred, label, path='', file_name=''):
    ''' Plots the area under roc curve for each kfold and its mean then saves it in a file

    Parameters
    ----------
    model_pred : dict
        A dict that follows prediction[<FOLD>][<METRICS>] : int, float, list
    label : str
        Defines a identification label for the plot
    path : str
    file_name : str
    features_to_show : int
        Max number of features on the plot

    The path of the file is <PATH>/importances_bar/<FILE_NAME>.png
    '''
    file_path = '%s/graphs' % path
    mkdir_p(file_path)
    file_path = '%s/%s.png' % (file_path, file_name)
    tprs = []
    aucs = []
    mean_fpr = np.linspace(0, 1, 100)
    kfolds_pred = model_pred['kfolds']

    for i, kfold in enumerate(kfolds_pred):
        metrics = metrics_by_prediction(kfolds_pred[kfold]['y_true'],
                                        kfolds_pred[kfold]['y_pred'])
        fpr, tpr = metrics['fpr_roc'], metrics['tpr_roc']
        tprs.append(interp(mean_fpr, fpr, tpr))
        tprs[-1][0] = 0.0
        roc_auc = metrics['roc_auc']
        aucs.append(roc_auc)
        plt.plot(fpr, tpr, lw=1, alpha=0.3)

    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',
             label='Chance', alpha=.8)

    mean_tpr = np.mean(tprs, axis=0)
    mean_tpr[-1] = 1.0
    mean_auc = np.mean(aucs)
    std_auc = np.std(aucs)
    plt.plot(mean_fpr, mean_tpr, color='b',
             label=r'Mean ROC (AUC = %0.3f $\pm$ %0.3f)' % (mean_auc, std_auc),
             lw=2, alpha=.8)

    plt.xlim([-0.05, 1.05])
    plt.ylim([-0.05, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('%s - ROC Curve' % label)
    plt.legend(loc="lower right")
    plt.savefig(file_path, dpi=250)
    ## Destroy plot so that it wont be overlaid with the next plot
    plt.close()
    ## If you want to show the figure in a window:
    # plt.show()


def mkdir_p(mypath):
    '''Creates a directory. equivalent to using mkdir -p on the command line'''
    try:
        makedirs(mypath)
    except OSError as exc:  # Python >2.5
        if exc.errno == EEXIST and path.isdir(mypath):
            pass
        else:
            raise

lista_de_vertice = ['a', 'b', 'c', 'd', 'e', 'f']

lista_de_aresta = [('a', 'b'), ('b', 'a'), ('b', 'c'), ('c', 'b'), ('c', 'e'), ('e', 'a'), ('e', 'b'), ('e', 'c'), ('e', 'd'), ('d', 'a'), ('d', 'e'), ('e', 'f'), ('f', 'e'), ('b', 'e'), ('a', 'd'), ('a', 'e')]
  
def cria_grafo(lista_de_vertices, lista_de_arestas):
    grafo = {}
    for vertice in lista_de_vertices:
        grafo[vertice] = []
    for aresta in lista_de_arestas:
        grafo[aresta[0]].append(aresta[1])
    return grafo

m = cria_grafo(lista_de_vertice, lista_de_aresta)

m

!pip3 install prediction